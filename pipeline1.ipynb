{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nibabel \n",
    "import subprocess\n",
    "import nilearn as nlr\n",
    "import nipy\n",
    "\n",
    "from nilearn._utils.compat import _basestring\n",
    "import glob\n",
    "\n",
    "from sklearn.externals.joblib import Parallel, delayed, Memory\n",
    "from nipype.caching import Memory as NipypeMemory\n",
    "import nipype.interfaces.spm as spm\n",
    "\n",
    "from nipy.modalities.fmri.glm import FMRILinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options = {'protocol': 'SOCIAL', \n",
    "           'dataset_description': 'HCP SOCIAL experiment', \n",
    "#            'dataset_dir': 'C:/data/graduate_thesis/hcp_data_test_gz/',  \n",
    "           'dataset_dir': 'C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\',  \n",
    "           'subject_dirs': '*', \n",
    "           'output_dir': 'output', \n",
    "#            'session_1_func': 'MNINonLinear/Results/tfMRI_%protocol%_RL/tfMRI_%protocol%_RL.nii.gz', \n",
    "#            'session_2_func': 'MNINonLinear/Results/tfMRI_%protocol%_LR/tfMRI_%protocol%_LR.nii.gz', \n",
    "           'session_2_func': 'MNINonLinear\\\\Results\\\\tfMRI_SOCIAL_RL\\\\tfMRI_SOCIAL_RL.nii.gz', \n",
    "           'session_1_func': 'MNINonLinear\\\\Results\\\\tfMRI_SOCIAL_LR\\\\tfMRI_SOCIAL_LR.nii.gz', \n",
    "           'caching': True, \n",
    "           'deleteorient': False, \n",
    "           'disable_distortion_correction': True, \n",
    "           'disable_slice_timing': True, \n",
    "           'TR': 0.72, \n",
    "           'TA': 'TR * (1 - 1 / nslices)', \n",
    "           'slice_order': 'ascending', \n",
    "           'interleaved': False, \n",
    "           'refslice': '1', \n",
    "           'slice_timing_software': 'spm', \n",
    "           'disable_realign': True, \n",
    "           'register_to_mean': True, \n",
    "           'realign_reslice': False, \n",
    "           'realign_software': 'spm', \n",
    "           'disable_coregister': True, \n",
    "           'coreg_func_to_anat': True, \n",
    "           'coregister_reslice': False, \n",
    "           'coregister_software': 'spm', \n",
    "           'disable_segment': True,\n",
    "           'segment_software': 'spm', \n",
    "           'newsegment': True, \n",
    "           'disable_normalize': True, \n",
    "           'template': 'MNI', \n",
    "           'func_write_voxel_sizes': [2.0, 2.0, 2.0], \n",
    "           'anat_write_voxel_size': [1.0, 1.0, 1.0], \n",
    "           'dartel': False, \n",
    "           'normalize_software': 'spm', \n",
    "           'fwhm': 4.0, \n",
    "           'smooth_software': 'spm',\n",
    "           'report': False, \n",
    "           'plot_tsdiffana': True,\n",
    "           'slicer': 'ortho',   # slicer of activation maps QA\n",
    "           'cut_coords': None, \n",
    "           'threshold': 3, \n",
    "           'cluster_th': 15,   # minimum number of voxels in reported clusters\n",
    "           'n_jobs': 32,\n",
    "           'scratch': 'scratch', \n",
    "#            'spm_dir': 'C:/Program Files/MATLAB/spm12' \n",
    "           'spm_dir': 'C:\\\\Program Files\\\\MATLAB\\\\spm12', \n",
    "#            'matlab_exec': 'C:/Program Files/MATLAB/R2017a'\n",
    "           'matlab_exec': 'C:\\\\Program Files\\\\MATLAB\\\\R2017a', \n",
    "           'nsubjects':3\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert options['dataset_dir']\n",
    "assert options['output_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expand_path(path, relative_to=None):\n",
    "    # cd to reference directory\n",
    "    if relative_to is None:\n",
    "        relative_to = os.getcwd()\n",
    "    else:\n",
    "        relative_to = expand_path(relative_to)\n",
    "        if not os.path.exists(relative_to):\n",
    "            raise OSError(\n",
    "                \"Reference path %s doesn't exist\" % relative_to)\n",
    "    old_cwd = os.getcwd()\n",
    "    os.chdir(relative_to)\n",
    "\n",
    "    _path = path\n",
    "#     if _path.startswith('..'):\n",
    "#         if _path == '..':\n",
    "#             _path = os.path.dirname(os.getcwd())\n",
    "#         else:\n",
    "#             match = re.match('(?P<head>(?:\\.{2}\\/)+)(?P<tail>.*)', _path)\n",
    "#             if match:\n",
    "#                 _path = os.getcwd()\n",
    "#                 for _ in range(len(match.group('head')) // 3):\n",
    "#                     _path = os.path.dirname(_path)\n",
    "#                 _path = os.path.join(_path, match.group('tail'))\n",
    "#             else:\n",
    "#                 _path = None\n",
    "#     elif _path.startswith('./'): #'.\\\\'\n",
    "#         _path = _path[2:]\n",
    "#     elif _path.startswith('.'):\n",
    "#         _path = _path[1:]\n",
    "#     elif _path.startswith('~'):\n",
    "#         if _path == '~':\n",
    "#             _path = os.environ['HOME']\n",
    "#         else:\n",
    "#             _path = os.path.join(os.environ['HOME'], _path[2:])\n",
    "\n",
    "    if not _path is None:\n",
    "        _path = os.path.abspath(_path)\n",
    "\n",
    "    # restore cwd\n",
    "    os.chdir(old_cwd)\n",
    "    return _path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_relative_path(ancestor, descendant):\n",
    "    if ancestor == descendant:\n",
    "        return \"\"\n",
    "\n",
    "    ancestor = ancestor.rstrip(\"\\\\\")\n",
    "    descendant = descendant.rstrip(\"\\\\\")\n",
    "    right_part = descendant[len(ancestor):].lstrip(\"\\\\\")\n",
    "    if right_part is None:\n",
    "        return None\n",
    "    else:\n",
    "        return right_part\n",
    "    \n",
    "    \n",
    "def get_abspath_relative_to_file(filename, ref_filename):\n",
    "    \n",
    "    assert os.path.isfile(ref_filename)\n",
    "\n",
    "    old_cwd = os.getcwd()  # save CWD\n",
    "    os.chdir(os.path.dirname(ref_filename))  # in context\n",
    "    abspath = os.path.abspath(filename)  # bing0!\n",
    "    os.chdir(old_cwd)  # restore CWD\n",
    "\n",
    "    return abspath\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\n"
     ]
    }
   ],
   "source": [
    "# check dataset_dir\n",
    "dataset_dir = expand_path(options['dataset_dir'])\n",
    "print(dataset_dir)\n",
    "if not os.path.isdir(dataset_dir):\n",
    "    raise OSError(\"dataset_dir '%s' doesn't exist\" % dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check output_dir\n",
    "output_dir = expand_path(options['output_dir'], relative_to=dataset_dir)\n",
    "if output_dir is None:\n",
    "    raise OSError(\n",
    "        (\"Could not expand 'output_dir' : invalid\"\n",
    "         \" path %s (relative to directory %s)\") % (options['output_dir'],\n",
    "                                                   dataset_dir))\n",
    "\n",
    "# check scratch\n",
    "scratch = expand_path(options['scratch'], relative_to=dataset_dir)\n",
    "if scratch is None:\n",
    "    raise OSError(\n",
    "        (\"Could not expand 'scratch' : invalid\"\n",
    "         \" path %s (relative to directory %s)\") % (options['scratch'],\n",
    "                                                   dataset_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\*\n"
     ]
    }
   ],
   "source": [
    "# check subject_dirs\n",
    "subject_dirs = expand_path(options['subject_dirs'], relative_to=dataset_dir)\n",
    "print(subject_dirs)\n",
    "if subject_dirs is None:\n",
    "    raise OSError(\n",
    "        (\"Could not expand 'output_dir' : invalid\"\n",
    "         \" path %s (relative to directory %s)\") % (options['subject_dirs'],\n",
    "                                                   dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preproc parameters\n",
    "preproc_params = {\n",
    "    'spm_dir': options['spm_dir'],\n",
    "    'matlab_exec': options['matlab_exec'],\n",
    "    'report': options['report'],\n",
    "    'output_dir': options['output_dir'],\n",
    "    'scratch': options['scratch'],\n",
    "    'dataset_id': options['dataset_dir'],\n",
    "    'n_jobs': options['n_jobs'],\n",
    "    'caching': options['caching'],\n",
    "    'tsdiffana': options['plot_tsdiffana'],\n",
    "    'dataset_description': options['dataset_description'],\n",
    "    'slice_timing_software': options['slice_timing_software'],\n",
    "    'realign_software': options['realign_software'],\n",
    "    'coregister_software': options['coregister_software'],\n",
    "    'smooth_software': options['smooth_software'], \n",
    "    'deleteorient': options['deleteorient'], \n",
    "    'slice_timing': options['disable_slice_timing']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if preproc_params['slice_timing']:\n",
    "    preproc_params.update(dict((k, options.get(k, None))\n",
    "                               for k in ['TR', 'TA', 'slice_order',\n",
    "                                         'interleaved']))\n",
    "    if preproc_params['TR'] is None:\n",
    "        preproc_params['slice_timing'] = False\n",
    "        \n",
    "# configure motion correction node\n",
    "preproc_params['realign'] = not options.get('disable_realign', False)\n",
    "if preproc_params['realign']:\n",
    "    preproc_params['realign_reslice'] = options.get('reslice_realign',\n",
    "                                                    False)\n",
    "    preproc_params['register_to_mean'] = options.get('register_to_mean',\n",
    "                                                     True)\n",
    "\n",
    "# configure coregistration node\n",
    "preproc_params['coregister'] = not options.get('disable_coregister',\n",
    "                                               False)\n",
    "if preproc_params['coregister']:\n",
    "    preproc_params['coregister_reslice'] = options.get(\n",
    "        'coregister_reslice')\n",
    "    preproc_params['coreg_anat_to_func'] = not options.get(\n",
    "        'coreg_func_to_anat', True)\n",
    "\n",
    "# configure tissue segmentation node\n",
    "preproc_params['segment'] = not options.get('disable_segment', False)\n",
    "preproc_params['newsegment'] = options.get(\n",
    "    'newsegment', False) and preproc_params['segment']\n",
    "\n",
    "# configure normalization node\n",
    "preproc_params['normalize'] = not options.get(\n",
    "    'disable_normalize', False)\n",
    "\n",
    "\n",
    "# configure output voxel sizes\n",
    "for brain in ['func', 'anat']:\n",
    "    k = '%s_write_voxel_size' % brain\n",
    "    ks = k + 's'\n",
    "    if k in options:\n",
    "        assert not ks in options, (\n",
    "            'Both %s and %s specified in ini file. Please use only one of '\n",
    "            'them, they mean thesame thing!')\n",
    "        options[ks] = options.pop(k)\n",
    "    preproc_params[ks] = options.get(\n",
    "        ks, [[3, 3, 3], [1, 1, 1]][brain == 'anat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure dartel\n",
    "preproc_params['dartel'] = options.get('dartel', False)\n",
    "preproc_params['output_modulated_tpms'] = options.get(\n",
    "    'output_modulated_tpms', False)\n",
    "\n",
    "# can't do dartel without newsegment!\n",
    "if not preproc_params['newsegment']:\n",
    "    preproc_params['newsegment'] = preproc_params['dartel']\n",
    "\n",
    "# configure smoothing node\n",
    "preproc_params['fwhm'] = options.get('fwhm', 0.)\n",
    "preproc_params['anat_fwhm'] = options.get('anat_fwhm', 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many subjects ?\n",
    "subjects = []\n",
    "nsubjects = options.get('nsubjects', np.inf)\n",
    "exclude_these_subject_ids = options.get(\n",
    "    'exclude_these_subject_ids', [])\n",
    "include_only_these_subject_ids = options.get(\n",
    "    'include_only_these_subject_ids', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subject_data_dirs = options.get(\"subject_dirs\", \"*\")\n",
    "if isinstance(subject_dirs, _basestring):\n",
    "    subject_dir_wildcard = os.path.join(dataset_dir, subject_dirs)\n",
    "    subject_data_dirs = [x for x in sorted(glob.glob(subject_dir_wildcard))\n",
    "                         if os.path.isdir(x)]\n",
    "    subject_data_dirs = [os.path.join(x, re.findall(r'\\d{6}', x)[0]) for x in subject_data_dirs]\n",
    "else:\n",
    "    # list of subjects or subject wildcards\n",
    "    subject_data_dirs = [os.path.join(dataset_dir, x)\n",
    "                         for x in subject_data_dirs]\n",
    "    subject_dir_wildcard = subject_data_dirs\n",
    "    aux = []\n",
    "    for subject_data_dir in subject_data_dirs:\n",
    "        for x in sorted(glob.glob(subject_data_dir)):\n",
    "            if os.path.isdir(x):\n",
    "                aux.append(x)\n",
    "    subject_data_dirs = aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\100307_3T_tfMRI_SOCIAL_preproc\\\\100307',\n",
       " 'C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\103414_3T_tfMRI_SOCIAL_preproc\\\\103414',\n",
       " 'C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\105115_3T_tfMRI_SOCIAL_preproc\\\\105115']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_data_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if not subject_data_dirs:\n",
    "    warnings.warn(\"No subject directories found for wildcard: %s\" % (\n",
    "        subject_dir_wildcard))\n",
    "else:\n",
    "    print(len(subject_data_dirs) == nsubjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_func_wildcards = [key for key in list(options.keys())\n",
    "                           if re.match(\"session_.+_func\", key)]\n",
    "sess_onset_wildcards = [key for key in list(options.keys())\n",
    "                        if re.match(\"session_.+_onset\", key)]\n",
    "sess_ids = [re.match(\"session_(.+)_func\", session).group(1)\n",
    "                for session in sess_func_wildcards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['session_1_func', 'session_2_func']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess_func_wildcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SubjectData(object):\n",
    "    def __init__(self, func=None, anat=None, subject_id=\"100307\",\n",
    "                 session_ids=None, output_dir=None, session_output_dirs=None,\n",
    "                 anat_output_dir=None, scratch=None, warpable=None, **kwargs):\n",
    "        if warpable is None:\n",
    "            warpable = ['anat', 'func']\n",
    "        self.func = func\n",
    "        self.anat = anat\n",
    "        self.subject_id = subject_id\n",
    "        self.session_ids = session_ids\n",
    "        self.n_sessions = None\n",
    "        self.output_dir = output_dir\n",
    "        self.anat_output_dir = anat_output_dir\n",
    "        self.session_output_dirs = session_output_dirs\n",
    "        self.warpable = warpable\n",
    "        self.failed = False\n",
    "        self.warpable = warpable\n",
    "        self.nipype_results = {}\n",
    "        self._set_items(**kwargs)\n",
    "        self.scratch = output_dir if scratch is None else scratch\n",
    "        self.anat_scratch_dir = anat_output_dir if scratch is None else scratch\n",
    "        self.session_scratch_dirs = (session_output_dirs if scratch is None\n",
    "                                     else [scratch] * len(session_output_dirs))\n",
    "\n",
    "    def _set_items(self, **kwargs):\n",
    "        for k, v in list(kwargs.items()):\n",
    "            setattr(self, k, v)\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "    \n",
    "    def _sanitize_output_dir(self, output_dir):\n",
    "        if output_dir is not None:\n",
    "            output_dir = os.path.abspath(output_dir)\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "        return output_dir\n",
    "\n",
    "    def _sanitize_session_output_dirs(self):\n",
    "        if self.session_output_dirs is None:\n",
    "            if self.n_sessions is None:\n",
    "                return\n",
    "            self.session_output_dirs = [None] * self.n_sessions\n",
    "\n",
    "        # session-wise func output directories\n",
    "        for sess, sess_output_dir in enumerate(self.session_output_dirs):\n",
    "            if sess_output_dir is None:\n",
    "                if self.n_sessions > 1:\n",
    "                    sess_output_dir = os.path.join(\n",
    "                        self.output_dir, self.session_ids[sess])\n",
    "                else:\n",
    "                    sess_output_dir = self.output_dir\n",
    "            else:\n",
    "                sess_output_dir = sess_output_dir\n",
    "            self.session_output_dirs[sess] = self._sanitize_output_dir(\n",
    "                sess_output_dir)\n",
    "\n",
    "    def _sanitize_session_scratch_dirs(self):\n",
    "        if self.session_scratch_dirs is None:\n",
    "            if self.n_sessions is None:\n",
    "                return\n",
    "            self.session_scratch_dirs = [None] * self.n_sessions\n",
    "\n",
    "        # session-wise func scratch directories\n",
    "        for sess, sess_scratch_dir in enumerate(self.session_scratch_dirs):\n",
    "            if sess_scratch_dir is None:\n",
    "                if self.n_sessions > 1:\n",
    "                    sess_scratch_dir = os.path.join(\n",
    "                        self.scratch, self.session_ids[sess])\n",
    "                else:\n",
    "                    sess_scratch_dir = self.scratch\n",
    "            self.session_scratch_dirs[sess] = self._sanitize_output_dir(\n",
    "                sess_scratch_dir)\n",
    "\n",
    "    def _sanitize_output_dirs(self):\n",
    "        # output dir\n",
    "        self.output_dir = self._sanitize_output_dir(self.output_dir)\n",
    "\n",
    "        # anat output dir\n",
    "        if self.anat_output_dir is None:\n",
    "            self.anat_output_dir = self.output_dir\n",
    "        self.anat_output_dir = self._sanitize_output_dir(self.anat_output_dir)\n",
    "\n",
    "        # sanitize per-session func output dirs\n",
    "        self._sanitize_session_output_dirs()\n",
    "\n",
    "    def _sanitize_scratch_dirs(self):\n",
    "        # scratch dir\n",
    "        self.scratch = self._sanitize_output_dir(self.scratch)\n",
    "\n",
    "        # anat scratch dir\n",
    "        if self.anat_scratch_dir is None:\n",
    "            self.anat_scratch_dir = self.scratch\n",
    "        self.anat_scratch_dir =\\\n",
    "            self._sanitize_output_dir(self.anat_scratch_dir)\n",
    "\n",
    "        # sanitize per-session func scratch dirs\n",
    "        self._sanitize_session_scratch_dirs()\n",
    "\n",
    "    def _niigz2nii(self):\n",
    "        if self.scratch is None:\n",
    "            self.scratch = self.output_dir\n",
    "        cache_dir = os.path.join(self.scratch, 'cache_dir')\n",
    "        mem = Memory(cache_dir, verbose=100)\n",
    "        self._sanitize_session_output_dirs()\n",
    "        self._sanitize_session_scratch_dirs()\n",
    "        if None not in [self.func, self.n_sessions,\n",
    "                        self.session_scratch_dirs]:\n",
    "            self.func = [mem.cache(do_niigz2nii)(\n",
    "                self.func[sess], output_dir=self.session_scratch_dirs[sess])\n",
    "                for sess in range(self.n_sessions)]\n",
    "        if self.anat is not None:\n",
    "            self.anat = mem.cache(do_niigz2nii)(\n",
    "                self.anat, output_dir=self.anat_scratch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\100307_3T_tfMRI_SOCIAL_preproc\\100307\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\103414_3T_tfMRI_SOCIAL_preproc\\103414\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\105115_3T_tfMRI_SOCIAL_preproc\\105115\n"
     ]
    }
   ],
   "source": [
    "for subject_data_dir in subject_data_dirs:\n",
    "    print(subject_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# os.path.join(subject_data_dirs[0],options[sess_func_wildcards[0]])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100307\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\100307_3T_tfMRI_SOCIAL_preproc\\100307\\MNINonLinear\\Results\\tfMRI_SOCIAL_LR\\tfMRI_SOCIAL_LR.nii.gz\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\100307_3T_tfMRI_SOCIAL_preproc\\100307\\MNINonLinear\\Results\\tfMRI_SOCIAL_RL\\tfMRI_SOCIAL_RL.nii.gz\n",
      "103414\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\103414_3T_tfMRI_SOCIAL_preproc\\103414\\MNINonLinear\\Results\\tfMRI_SOCIAL_LR\\tfMRI_SOCIAL_LR.nii.gz\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\103414_3T_tfMRI_SOCIAL_preproc\\103414\\MNINonLinear\\Results\\tfMRI_SOCIAL_RL\\tfMRI_SOCIAL_RL.nii.gz\n",
      "105115\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\105115_3T_tfMRI_SOCIAL_preproc\\105115\\MNINonLinear\\Results\\tfMRI_SOCIAL_LR\\tfMRI_SOCIAL_LR.nii.gz\n",
      "C:\\data\\graduate_thesis\\hcp_data_test_gz\\105115_3T_tfMRI_SOCIAL_preproc\\105115\\MNINonLinear\\Results\\tfMRI_SOCIAL_RL\\tfMRI_SOCIAL_RL.nii.gz\n"
     ]
    }
   ],
   "source": [
    "for subject_data_dir in subject_data_dirs:\n",
    "    if len(subjects) == nsubjects:\n",
    "        # we've had enough subjects already; end\n",
    "        break\n",
    "    subject_id = os.path.basename(subject_data_dir)\n",
    "    print(subject_id)\n",
    "    subject_output_dir = os.path.join(output_dir, subject_id)\n",
    "    if scratch is not None:\n",
    "        subject_scratch = os.path.join(scratch, subject_id)\n",
    "    else:\n",
    "        subject_scratch = None\n",
    "    \n",
    "    \n",
    "    # grab functional data\n",
    "    func = []\n",
    "    sess_output_dirs = []\n",
    "    # skip_subject = False\n",
    "    onset = []\n",
    "    \n",
    "    for s, sess_func_wildcard in enumerate(sess_func_wildcards):\n",
    "        o = None\n",
    "        if s < len(sess_onset_wildcards):\n",
    "            sess_onset_wildcard = sess_onset_wildcards[s]\n",
    "            sess_onset_wildcard = options[sess_onset_wildcard]\n",
    "            sess_onset_wildcard = os.path.join(subject_data_dir,\n",
    "                                               sess_onset_wildcard)\n",
    "            sess_onset = sorted(glob.glob(sess_onset_wildcard))\n",
    "            if len(sess_onset) > 1:\n",
    "                raise ValueError\n",
    "            if len(sess_onset) > 0:\n",
    "                o = sess_onset[0]\n",
    "        onset.append(o)\n",
    "        \n",
    "        sess_func_wildcard = options[sess_func_wildcard]  \n",
    "        \n",
    "#         if expand_path(sess_func_wildcard, relative_to=dataset_dir) is None:\n",
    "#             raise OSError(\n",
    "#                 (\"Could not expand 'sess_func_wildcard' : invalid\"\n",
    "#                  \" path %s (relative to directory %s)\") % (options['sess_func_wildcard'],\n",
    "#                                                            dataset_dir))\n",
    "#         else: \n",
    "#         subject_data_dir = os.path.join(subject_data_dir,\n",
    "#                                           sess_func_wildcard) \n",
    "        \n",
    "#         sess_func = os.path.join(subject_data_dir,\n",
    "#                                           sess_func_wildcard)    \n",
    "        sess_func = os.path.join(subject_data_dir,\n",
    "                                          sess_func_wildcard) \n",
    "        \n",
    "        # sess_func = sorted(glob.glob(sess_func_wildcard))\n",
    "    \n",
    "        # skip session if no data found\n",
    "        if not sess_func:\n",
    "            warnings.warn(\n",
    "                (\"subject %s: No func images found for\"\n",
    "                 \" wildcard %s\" % (subject_id, sess_func_wildcard)))\n",
    "            continue\n",
    "        \n",
    "        sess_dir = os.path.dirname(sess_func)\n",
    "#         if len(sess_func) == 1:\n",
    "#             sess_func = sess_func_wildcard[0]\n",
    "        func.append(sess_func)\n",
    "        print(sess_func)\n",
    "\n",
    "        # session output dir\n",
    "        if os.path.basename(sess_dir) != os.path.basename(\n",
    "                subject_output_dir):\n",
    "            sess_output_dir = os.path.join(subject_output_dir,\n",
    "                                           get_relative_path(\n",
    "                                               subject_data_dir, sess_dir))\n",
    "        else:\n",
    "            sess_output_dir = subject_output_dir\n",
    "        if not os.path.exists(sess_output_dir):\n",
    "            os.makedirs(sess_output_dir)\n",
    "        sess_output_dirs.append(sess_output_dir)\n",
    "        \n",
    "\n",
    "#     # something is wrong with this guy, skip\n",
    "#     if skip_subject:\n",
    "#         warnings.warn(\"Skipping subject %s\" % subject_id)\n",
    "#         continue\n",
    "\n",
    "    # grab anat\n",
    "    anat = None\n",
    "    if not options.get(\"anat\", None) is None:\n",
    "        # grap anat file(s)\n",
    "        anat_wildcard = os.path.join(subject_data_dir, options['anat'])\n",
    "        anat = glob.glob(anat_wildcard)\n",
    "        # skip subject if anat absent\n",
    "        if len(anat) < 1:\n",
    "            print((\n",
    "                \"subject %s: anat image matching %s not found!; skipping\"\n",
    "                \" subject\" % (subject_id, anat_wildcard)))\n",
    "            continue\n",
    "\n",
    "        # we need just 1 anat volume\n",
    "        anat = anat[0]\n",
    "        anat_dir = os.path.dirname(anat)\n",
    "    else:\n",
    "        anat = None\n",
    "        anat_dir = \"\"\n",
    "\n",
    "    # anat output dir\n",
    "    anat_output_dir = None\n",
    "    if anat_dir:\n",
    "        anat_output_dir = os.path.join(subject_output_dir,\n",
    "                                       get_relative_path(subject_data_dir,\n",
    "                                                         anat_dir))\n",
    "        if not os.path.exists(anat_output_dir):\n",
    "            os.makedirs(anat_output_dir)\n",
    "\n",
    "    # make subject data\n",
    "    subject_data = SubjectData(\n",
    "        subject_id=subject_id, \n",
    "        func=func, \n",
    "        anat=anat,\n",
    "        output_dir=subject_output_dir,\n",
    "        scratch=subject_scratch,\n",
    "        session_output_dirs=sess_output_dirs,\n",
    "        anat_output_dir=anat_output_dir,\n",
    "        session_id=sess_ids,\n",
    "        data_dir=subject_data_dir,\n",
    "        onset=onset,\n",
    "        TR=options.get('TR', None),\n",
    "        drift_model='Cosine',\n",
    "        hrf_model=options.get('hrf_model', 'spm + derivative'),\n",
    "        hfcut=options.get(\"hfcut\", 128.),\n",
    "        time_units=options.get(\"time_units\", \"seconds\"))\n",
    "\n",
    "    subjects.append(subject_data)\n",
    "    \n",
    "if not subjects:\n",
    "    warnings.warn(\n",
    "        \"No subjects globbed (dataset_dir=%s, subject_dir_wildcard=%s\" % (\n",
    "            dataset_dir, subject_dir_wildcard))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\100307_3T_tfMRI_SOCIAL_preproc\\\\100307'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects[0].data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slicer = 'ortho'  # slicer of activation maps QA\n",
    "cut_coords = None\n",
    "threshold = 3.\n",
    "cluster_th = 15  # minimum number of voxels in reported clusters\n",
    "protocol = 'SOCIAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regex for contrasts\n",
    "CON_REAL_REGX = (\"set fmri\\(con_real(?P<con_num>\\d+?)\\.(?P<ev_num>\\d+?)\\)\"\n",
    "            \" (?P<con_val>\\S+)\")\n",
    "\n",
    "# regex for \"Number of EVs\"\n",
    "NUM_EV_REGX = \"\"\"set fmri\\(evs_orig\\) (?P<evs_orig>\\d+)\n",
    "set fmri\\(evs_real\\) (?P<evs_real>\\d+)\n",
    "set fmri\\(evs_vox\\) (?P<evs_vox>\\d+)\"\"\"\n",
    "\n",
    "# regex for \"Number of contrasts\"\n",
    "NUM_CON_REGX = \"\"\"set fmri\\(ncon_orig\\) (?P<ncon>\\d+)\n",
    "set fmri\\(ncon_real\\) (?P<ncon_real>\\d+)\"\"\"\n",
    "\n",
    "# regex for \"# EV %i title\"\n",
    "EV_TITLE_REGX = \"\"\"set fmri\\(evtitle\\d+?\\) \\\"(?P<evtitle>.+)\\\"\"\"\"\n",
    "\n",
    "# regex for \"Title for contrast_real %i\"\n",
    "CON_TITLE_REGX = \"\"\"set fmri\\(conname_real\\.\\d+?\\) \\\"(?P<conname_real>.+)\\\"\"\"\"\n",
    "\n",
    "# regex for \"Basic waveform shape (EV %i)\"\n",
    "# 0 : Square\n",
    "# 1 : Sinusoid\n",
    "# 2 : Custom (1 entry per volume)\n",
    "# 3 : Custom (3 column format)\n",
    "# 4 : Interaction\n",
    "# 10 : Empty (all zeros)\n",
    "EV_SHAPE_REGX = \"\"\"set fmri\\(shape\\d+\\) (?P<shape>[0|1|3])\"\"\"\n",
    "\n",
    "# regex for \"Custom EV file (EV %i)\"\n",
    "EV_CUSTOM_FILE_REGX = \"\"\"set fmri\\(custom\\d+?\\) \\\"(?P<custom>.+)\\\"\"\"\"\n",
    "\n",
    "\n",
    "def read_fsl_design_file(design_filename):\n",
    "    # read design file\n",
    "    design_conf = open(design_filename, 'r').read()\n",
    "\n",
    "    # scrape n_conditions and n_contrasts\n",
    "    n_conditions_orig = int(re.search(NUM_EV_REGX,\n",
    "                                      design_conf).group(\"evs_orig\"))\n",
    "    n_conditions = int(re.search(NUM_EV_REGX, design_conf).group(\"evs_real\"))\n",
    "    n_contrasts = int(re.search(NUM_CON_REGX, design_conf).group(\"ncon_real\"))\n",
    "\n",
    "    # initialize 2D array of contrasts\n",
    "    contrasts = np.zeros((n_contrasts, n_conditions))\n",
    "\n",
    "    # lookup EV titles\n",
    "    conditions = [item.group(\"evtitle\") for item in re.finditer(\n",
    "                  EV_TITLE_REGX, design_conf)]\n",
    "    assert len(conditions) == n_conditions_orig\n",
    "\n",
    "    # lookup contrast titles\n",
    "    contrast_ids = [item.group(\"conname_real\")for item in re.finditer(\n",
    "                    CON_TITLE_REGX, design_conf)]\n",
    "    assert len(contrast_ids) == n_contrasts\n",
    "\n",
    "    # # lookup EV (condition) shapes\n",
    "    # condition_shapes = [int(item.group(\"shape\")) for item in re.finditer(\n",
    "    #         EV_SHAPE_REGX, design_conf)]\n",
    "    # print(condition_shapes)\n",
    "\n",
    "    # lookup EV (condition) custom files\n",
    "    timing_files = [get_abspath_relative_to_file(item.group(\"custom\"),\n",
    "                                                  design_filename)\n",
    "                    for item in re.finditer(EV_CUSTOM_FILE_REGX, design_conf)]\n",
    "\n",
    "    # lookup the contrast values\n",
    "    count = 0\n",
    "    for item in re.finditer(CON_REAL_REGX, design_conf):\n",
    "        count += 1\n",
    "        value = float(item.group('con_val'))\n",
    "\n",
    "        i = int(item.group('con_num')) - 1\n",
    "        j = int(item.group('ev_num')) - 1\n",
    "\n",
    "        # roll-call\n",
    "        assert 0 <= i < n_contrasts, item.group()\n",
    "        assert 0 <= j < n_conditions, item.group()\n",
    "\n",
    "        contrasts[i, j] = value\n",
    "\n",
    "    # roll-call\n",
    "    assert count == n_contrasts * n_conditions, count\n",
    "\n",
    "    return conditions, timing_files, contrast_ids, contrasts\n",
    "\n",
    "\n",
    "def niigz2nii(ifilename, output_dir=None):\n",
    "    if isinstance(ifilename, list):\n",
    "        return [niigz2nii(x, output_dir=output_dir) for x in ifilename]\n",
    "    else:\n",
    "        if not isinstance(ifilename, _basestring):\n",
    "            raise RuntimeError(\n",
    "                \"ifilename must be string or list of strings, got %s\" % type(\n",
    "                    ifilename))\n",
    "\n",
    "    if not ifilename.endswith('.nii.gz'):\n",
    "        return ifilename\n",
    "\n",
    "    ofilename = ifilename[:-3]\n",
    "    if not output_dir is None:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        ofilename = os.path.join(output_dir, os.path.basename(ofilename))\n",
    "\n",
    "    nibabel.save(nibabel.load(ifilename), ofilename)\n",
    "\n",
    "    return ofilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pypreprocess.external.nistats.design_matrix import make_design_matrix\n",
    "\n",
    "def make_paradigm_from_timing_files(timing_files, condition_ids=None):\n",
    "    if not condition_ids is None:\n",
    "        assert len(condition_ids) == len(timing_files)\n",
    "\n",
    "    onsets = []\n",
    "    durations = []\n",
    "    amplitudes = []\n",
    "    _condition_ids = []\n",
    "    count = 0\n",
    "    for timing_file in timing_files:\n",
    "        timing = np.loadtxt(timing_file)\n",
    "        if timing.ndim == 1:\n",
    "            timing = timing[np.newaxis, :]\n",
    "        if condition_ids is None:\n",
    "            condition_id = os.path.basename(timing_file).lower(\n",
    "                ).split('.')[0]\n",
    "            \n",
    "        else:\n",
    "            condition_id = condition_ids[count]\n",
    "        _condition_ids = _condition_ids + [condition_id\n",
    "                                           ] * timing.shape[0]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if timing.shape[1]  == 3:\n",
    "            onsets = onsets + list(timing[..., 0])\n",
    "            durations = durations + list(timing[..., 1])\n",
    "            amplitudes = amplitudes + list(timing[..., 2])\n",
    "        elif timing.shape[1]  == 2:\n",
    "            onsets = onsets + list(timing[..., 0])\n",
    "            durations = durations + list(timing[..., 1])\n",
    "            amplitudes = durations + list(np.ones(len(timing)))\n",
    "        elif timing.shape[1] == 1:\n",
    "            onsets = onsets + list(timing[..., 0])\n",
    "            durations = durations + list(np.zeros(len(timing)))\n",
    "            amplitudes = durations + list(np.ones(len(timing)))\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"Timing info must either be 1D array of onsets of 2D \"\n",
    "                \"array with 2 or 3 columns: the first column is for \"\n",
    "                \"the onsets, the second for the durations, and the \"\n",
    "                \"third --if present-- if for the amplitudes; got %s\" % timing)\n",
    "        print('--------')\n",
    "        print(onsets)\n",
    "        print(durations)\n",
    "        print(amplitudes)\n",
    "        print(_condition_ids)\n",
    "        print('--------')\n",
    "    return pd.DataFrame({'name': _condition_ids,\n",
    "                         'onset': onsets,\n",
    "                         'duration': durations,\n",
    "                         'modulation': amplitudes})\n",
    "\n",
    "\n",
    "def make_dmtx_from_timing_files(timing_files, condition_ids=None,\n",
    "                                frametimes=None, n_scans=None, tr=None,\n",
    "                                add_regs_file=None,\n",
    "                                add_reg_names=None,\n",
    "                                **make_dmtx_kwargs):\n",
    "    # make paradigm\n",
    "    paradigm = make_paradigm_from_timing_files(timing_files,\n",
    "                                               condition_ids=condition_ids)\n",
    "\n",
    "    # make frametimes\n",
    "    if frametimes is None:\n",
    "#         assert not n_scans is None, (\"frametimes not specified, especting a \"\n",
    "#                                      \"value for n_scans\")\n",
    "#         assert not tr is None, (\"frametimes not specified, especting a \"\n",
    "#                                 \"value for tr\")\n",
    "        frametimes = np.linspace(0, (n_scans - 1) * tr, n_scans)\n",
    "#     else:\n",
    "#         assert n_scans is None, (\"frametimes specified, not especting a \"\n",
    "#                                  \"value for n_scans\")\n",
    "#         assert tr is None, (\"frametimes specified, not especting a \"\n",
    "#                                  \"value for tr\")\n",
    "\n",
    "    # load addition regressors from file\n",
    "    if not add_regs_file is None:\n",
    "        if isinstance(add_regs_file, np.ndarray):\n",
    "            add_regs = add_regs_file\n",
    "        else:\n",
    "            assert os.path.isfile(add_regs_file), (\n",
    "                \"add_regs_file %s doesn't exist\")\n",
    "            add_regs = np.loadtxt(add_regs_file)\n",
    "        assert add_regs.ndim == 2, (\n",
    "            \"Bad add_regs_file: %s (must contain a 2D array, each column \"\n",
    "            \"representing the values of a single regressor)\" % add_regs_file)\n",
    "        if add_reg_names is None:\n",
    "            add_reg_names = [\"R%i\" % (col + 1) for col in range(\n",
    "                    add_regs.shape[-1])]\n",
    "        else:\n",
    "            assert len(add_reg_names) == add_regs.shape[1], (\n",
    "                \"Expecting %i regressor names, got %i\" % (\n",
    "                    add_regs.shape[1], len(add_reg_names)))\n",
    "\n",
    "        make_dmtx_kwargs[\"add_reg_names\"] = add_reg_names\n",
    "        make_dmtx_kwargs[\"add_regs\"] = add_regs\n",
    "\n",
    "    # make design matrix\n",
    "    design_matrix = make_design_matrix(frame_times=frametimes,\n",
    "                                       paradigm=paradigm,\n",
    "                                       **make_dmtx_kwargs)\n",
    "\n",
    "    # return output\n",
    "    return design_matrix, paradigm, frametimes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject_data.design_files = [os.path.join(\n",
    "            subject_data.data_dir, (\"MNINonLinear\\\\Results\\\\tfMRI_%s_%s\\\\\"\n",
    "                                    \"tfMRI_%s_%s_hp200_s4_level1.fsf\") % (\n",
    "                protocol, direction, protocol, direction))\n",
    "            for direction in ['LR', 'RL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fsl_condition_ids, timing_files, fsl_contrast_ids, contrast_values = \\\n",
    "            read_fsl_design_file(subject_data.design_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0., -1.,  0.],\n",
       "       [-1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., -1.,  0.],\n",
       "       [-1.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\105115_3T_tfMRI_SOCIAL_preproc\\\\105115\\\\MNINonLinear\\\\Results\\\\EVs\\\\rnd.txt',\n",
       " 'C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\105115_3T_tfMRI_SOCIAL_preproc\\\\105115\\\\MNINonLinear\\\\Results\\\\EVs\\\\mental.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RANDOM', 'TOM', 'RANDOM-TOM', 'neg_RANDOM', 'neg_TOM', 'TOM-RANDOM']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsl_contrast_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RANDOM', 'TOM']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsl_condition_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timing_files = [tf.replace(\"EVs\", \"tfMRI_%s_%s\\\\EVs\" % (\n",
    "                    \"SOCIAL\", \"LR\")) for tf in timing_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\105115_3T_tfMRI_SOCIAL_preproc\\\\105115\\\\MNINonLinear\\\\Results\\\\tfMRI_SOCIAL_LR\\\\EVs\\\\rnd.txt',\n",
       " 'C:\\\\data\\\\graduate_thesis\\\\hcp_data_test_gz\\\\105115_3T_tfMRI_SOCIAL_preproc\\\\105115\\\\MNINonLinear\\\\Results\\\\tfMRI_SOCIAL_LR\\\\EVs\\\\mental.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "[84.031000000000006, 160.08000000000001]\n",
      "[23.0, 23.0]\n",
      "[1.0, 1.0]\n",
      "['rnd', 'rnd']\n",
      "--------\n",
      "--------\n",
      "[84.031000000000006, 160.08000000000001, 8.2100000000000009, 46.006999999999998, 122.056]\n",
      "[23.0, 23.0, 23.0, 23.0, 23.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "['rnd', 'rnd', 'mental', 'mental', 'mental']\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>modulation</th>\n",
       "      <th>name</th>\n",
       "      <th>onset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rnd</td>\n",
       "      <td>84.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rnd</td>\n",
       "      <td>160.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mental</td>\n",
       "      <td>8.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mental</td>\n",
       "      <td>46.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mental</td>\n",
       "      <td>122.056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  modulation    name    onset\n",
       "0      23.0         1.0     rnd   84.031\n",
       "1      23.0         1.0     rnd  160.080\n",
       "2      23.0         1.0  mental    8.210\n",
       "3      23.0         1.0  mental   46.007\n",
       "4      23.0         1.0  mental  122.056"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paradigm = make_paradigm_from_timing_files(timing_files)\n",
    "paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "[84.031000000000006, 160.08000000000001]\n",
      "[23.0, 23.0]\n",
      "[1.0, 1.0]\n",
      "['RANDOM', 'RANDOM']\n",
      "--------\n",
      "--------\n",
      "[84.031000000000006, 160.08000000000001, 8.2100000000000009, 46.006999999999998, 122.056]\n",
      "[23.0, 23.0, 23.0, 23.0, 23.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "['RANDOM', 'RANDOM', 'TOM', 'TOM', 'TOM']\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "n = nibabel.load(subject_data.func[1]).shape[-1]\n",
    "design_matrix, paradigm, frametimes = make_dmtx_from_timing_files(\n",
    "            timing_files,fsl_condition_ids, n_scans = int(n), tr=0.72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_scans = nibabel.load(subject_data.func[1]).shape[-1]\n",
    "tr = 0.72\n",
    "a  = np.linspace(0, (n_scans - 1) * tr, n_scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.  ,    0.72,    1.44,    2.16,    2.88,    3.6 ,    4.32,\n",
       "          5.04,    5.76,    6.48,    7.2 ,    7.92,    8.64,    9.36,\n",
       "         10.08,   10.8 ,   11.52,   12.24,   12.96,   13.68,   14.4 ,\n",
       "         15.12,   15.84,   16.56,   17.28,   18.  ,   18.72,   19.44,\n",
       "         20.16,   20.88,   21.6 ,   22.32,   23.04,   23.76,   24.48,\n",
       "         25.2 ,   25.92,   26.64,   27.36,   28.08,   28.8 ,   29.52,\n",
       "         30.24,   30.96,   31.68,   32.4 ,   33.12,   33.84,   34.56,\n",
       "         35.28,   36.  ,   36.72,   37.44,   38.16,   38.88,   39.6 ,\n",
       "         40.32,   41.04,   41.76,   42.48,   43.2 ,   43.92,   44.64,\n",
       "         45.36,   46.08,   46.8 ,   47.52,   48.24,   48.96,   49.68,\n",
       "         50.4 ,   51.12,   51.84,   52.56,   53.28,   54.  ,   54.72,\n",
       "         55.44,   56.16,   56.88,   57.6 ,   58.32,   59.04,   59.76,\n",
       "         60.48,   61.2 ,   61.92,   62.64,   63.36,   64.08,   64.8 ,\n",
       "         65.52,   66.24,   66.96,   67.68,   68.4 ,   69.12,   69.84,\n",
       "         70.56,   71.28,   72.  ,   72.72,   73.44,   74.16,   74.88,\n",
       "         75.6 ,   76.32,   77.04,   77.76,   78.48,   79.2 ,   79.92,\n",
       "         80.64,   81.36,   82.08,   82.8 ,   83.52,   84.24,   84.96,\n",
       "         85.68,   86.4 ,   87.12,   87.84,   88.56,   89.28,   90.  ,\n",
       "         90.72,   91.44,   92.16,   92.88,   93.6 ,   94.32,   95.04,\n",
       "         95.76,   96.48,   97.2 ,   97.92,   98.64,   99.36,  100.08,\n",
       "        100.8 ,  101.52,  102.24,  102.96,  103.68,  104.4 ,  105.12,\n",
       "        105.84,  106.56,  107.28,  108.  ,  108.72,  109.44,  110.16,\n",
       "        110.88,  111.6 ,  112.32,  113.04,  113.76,  114.48,  115.2 ,\n",
       "        115.92,  116.64,  117.36,  118.08,  118.8 ,  119.52,  120.24,\n",
       "        120.96,  121.68,  122.4 ,  123.12,  123.84,  124.56,  125.28,\n",
       "        126.  ,  126.72,  127.44,  128.16,  128.88,  129.6 ,  130.32,\n",
       "        131.04,  131.76,  132.48,  133.2 ,  133.92,  134.64,  135.36,\n",
       "        136.08,  136.8 ,  137.52,  138.24,  138.96,  139.68,  140.4 ,\n",
       "        141.12,  141.84,  142.56,  143.28,  144.  ,  144.72,  145.44,\n",
       "        146.16,  146.88,  147.6 ,  148.32,  149.04,  149.76,  150.48,\n",
       "        151.2 ,  151.92,  152.64,  153.36,  154.08,  154.8 ,  155.52,\n",
       "        156.24,  156.96,  157.68,  158.4 ,  159.12,  159.84,  160.56,\n",
       "        161.28,  162.  ,  162.72,  163.44,  164.16,  164.88,  165.6 ,\n",
       "        166.32,  167.04,  167.76,  168.48,  169.2 ,  169.92,  170.64,\n",
       "        171.36,  172.08,  172.8 ,  173.52,  174.24,  174.96,  175.68,\n",
       "        176.4 ,  177.12,  177.84,  178.56,  179.28,  180.  ,  180.72,\n",
       "        181.44,  182.16,  182.88,  183.6 ,  184.32,  185.04,  185.76,\n",
       "        186.48,  187.2 ,  187.92,  188.64,  189.36,  190.08,  190.8 ,\n",
       "        191.52,  192.24,  192.96,  193.68,  194.4 ,  195.12,  195.84,\n",
       "        196.56])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def run_suject_level1_glm(subject_data,\n",
    "#                           readout_time=.01392,  # seconds\n",
    "#                           tr=.72,\n",
    "#                           dc=True,\n",
    "#                           hrf_model=\"spm + derivative\",\n",
    "#                           drift_model=\"Cosine\",\n",
    "#                           hfcut=100,\n",
    "#                           regress_motion=True,\n",
    "#                           slicer='ortho',\n",
    "#                           cut_coords=None,\n",
    "#                           threshold=3.,\n",
    "#                           cluster_th=15,\n",
    "#                           normalize=True,\n",
    "#                           fwhm=0.,\n",
    "#                           protocol=\"SOCIAL\",\n",
    "#                           func_write_voxel_sizes=None,\n",
    "#                           anat_write_voxel_sizes=None,\n",
    "#                           **other_preproc_kwargs\n",
    "#                           ):\n",
    "#     \"\"\"\n",
    "#     Function to do preproc + analysis for a single HCP subject (task fMRI)\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "# #     add_regs_files = None\n",
    "# #     n_regressions = 6\n",
    "#     subject_data.n_sessions = 2\n",
    "\n",
    "#     subject_data.tmp_output_dir = os.path.join(subject_data.output_dir, \"tmp\")\n",
    "#     if not os.path.exists(subject_data.tmp_output_dir):\n",
    "#         os.makedirs(subject_data.tmp_output_dir)\n",
    "\n",
    "#     if not os.path.exists(subject_data.output_dir):\n",
    "#         os.makedirs(subject_data.output_dir)\n",
    "\n",
    "#     mem = Memory(os.path.join(subject_data.output_dir, \"cache_dir\"),\n",
    "#                  verbose=100)\n",
    "\n",
    "#     # glob design files (.fsf)\n",
    "#     subject_data.design_files = [os.path.join(\n",
    "#             subject_data.data_dir, (\"MNINonLinear\\\\Results\\\\tfMRI_%s_%s\\\\\"\n",
    "#                                     \"tfMRI_%s_%s_hp200_s4_level1.fsf\") % (\n",
    "#                 protocol, direction, protocol, direction))\n",
    "#             for direction in ['LR', 'RL']]\n",
    "\n",
    "#     assert len(subject_data.design_files) == 2\n",
    "    \n",
    "#     for df in subject_data.design_files:\n",
    "#         if not os.path.isfile(df):\n",
    "#             return\n",
    "#     print(subject_data.design_files)\n",
    "#     if 0x0:\n",
    "#         subject_data = _do_fmri_distortion_correction(\n",
    "#             subject_data, dc=dc, fwhm=fwhm, readout_time=readout_time,\n",
    "#             **other_preproc_kwargs)\n",
    "\n",
    "#     # chronometry\n",
    "# #     stats_start_time = pretty_time()\n",
    "\n",
    "#     # merged lists\n",
    "#     paradigms = []\n",
    "#     frametimes_list = []\n",
    "#     design_matrices = []\n",
    "#     # fmri_files = []\n",
    "#     n_scans = []\n",
    "#     # for direction, direction_index in zip(['LR', 'RL'], xrange(2)):\n",
    "#     for sess in xrange(subject_data.n_sessions):\n",
    "#         direction = ['LR', 'RL'][sess]\n",
    "#         # glob the design file\n",
    "#         # design_file = os.path.join(# _subject_data_dir, \"tfMRI_%s_%s\" % (\n",
    "#                 # protocol, direction),\n",
    "#         design_file = subject_data.design_files[sess]\n",
    "#                 #                    \"tfMRI_%s_%s_hp200_s4_level1.fsf\" % (\n",
    "#                 # protocol, direction))\n",
    "#         if not os.path.isfile(design_file):\n",
    "#             print(\"Can't find design file %s; skipping subject %s\" %\n",
    "#                   design_file, subject_data.subject_id)\n",
    "#             return\n",
    "\n",
    "#         # read the experimental setup\n",
    "#         print(\"Reading experimental setup from %s ...\" % design_file)\n",
    "#         fsl_condition_ids, timing_files, fsl_contrast_ids, contrast_values = \\\n",
    "#             read_fsl_design_file(design_file)\n",
    "#         print(\"... done.\\r\\n\")\n",
    "\n",
    "#         # fix timing filenames\n",
    "#         timing_files = [tf.replace(\"EVs\", \"tfMRI_%s_%s\\\\EVs\" % (\n",
    "#                     protocol, direction)) for tf in timing_files]\n",
    "\n",
    "#         # make design matrix\n",
    "#         print(\"Constructing design matrix for direction %s ...\" % direction)\n",
    "#         _n_scans = nibabel.load(subject_data.func[sess]).shape[-1]\n",
    "#         n_scans.append(_n_scans)\n",
    "# #         add_regs_file = add_regs_files[\n",
    "# #             sess] if not add_regs_files is None else None\n",
    "#         design_matrix, paradigm, frametimes = make_dmtx_from_timing_files(\n",
    "#             timing_files, fsl_condition_ids, n_scans=_n_scans, tr=tr,\n",
    "#             hrf_model=hrf_model, drift_model=drift_model, hfcut=hfcut,\n",
    "#             add_regs_file=add_regs_file,\n",
    "#             add_reg_names= None\n",
    "#             )\n",
    "\n",
    "#         print(\"... done.\")\n",
    "#         paradigms.append(paradigm)\n",
    "#         frametimes_list.append(frametimes)\n",
    "#         design_matrices.append(design_matrix)\n",
    "\n",
    "#         # convert contrasts to dict\n",
    "#         contrasts = dict((contrast_id,\n",
    "#                           # append zeros to end of contrast to match design\n",
    "#                           np.hstack((contrast_value, np.zeros(len(\n",
    "#                                 design_matrix.names) - len(contrast_value)))))\n",
    "\n",
    "#                          for contrast_id, contrast_value in zip(\n",
    "#                 fsl_contrast_ids, contrast_values))\n",
    "\n",
    "\n",
    "#         contrasts = dict((k, v) for k, v in contrasts.items() if \"-\" in k)\n",
    "\n",
    "#     # replicate contrasts across sessions\n",
    "#     contrasts = dict((cid, [cval] * 2)\n",
    "#                      for cid, cval in contrasts.items())\n",
    "\n",
    "#     cache_dir = cache_dir = os.path.join(subject_data.output_dir,\n",
    "#                                          'cache_dir')\n",
    "#     if not os.path.exists(cache_dir):\n",
    "#         os.makedirs(cache_dir)\n",
    "#     nipype_mem = NipypeMemory(base_dir=cache_dir)\n",
    "\n",
    "#     if 0x0:\n",
    "#         if np.sum(fwhm) > 0.:\n",
    "#             subject_data.func = nipype_mem.cache(spm.Smooth)(\n",
    "#                 in_files=subject_data.func,\n",
    "#                 fwhm=fwhm,\n",
    "#                 ignore_exception=False,\n",
    "#                 ).outputs.smoothed_files\n",
    "\n",
    "#     # compute native-space maps and mask\n",
    "#     stuff = mem.cache(tortoise)(\n",
    "#         subject_data.func, subject_data.anat)\n",
    "#     if stuff is None:\n",
    "#         return None\n",
    "#     effects_maps, z_maps, mask_path, map_dirs = stuff\n",
    "\n",
    "#     # remove repeated contrasts\n",
    "#     contrasts = dict((cid, cval[0]) for cid, cval in contrasts.items())\n",
    "#     import json\n",
    "#     json.dump(dict((k, list(v)) for k, v in contrasts.items()),\n",
    "#               open(os.path.join(subject_data.tmp_output_dir,\n",
    "#                                 \"contrasts.json\"), \"w\"))\n",
    "#     subject_data.contrasts = contrasts\n",
    "\n",
    "#     if normalize:\n",
    "#         assert hasattr(subject_data, \"parameter_file\")\n",
    "\n",
    "#         subject_data.native_effects_maps = effects_maps\n",
    "#         subject_data.native_z_maps = z_maps\n",
    "#         subject_data.native_mask_path = mask_path\n",
    "\n",
    "#         # warp effects maps and mask from native to standard space (MNI)\n",
    "#         apply_to_files = [\n",
    "#             v for _, v in subject_data.native_effects_maps.items()\n",
    "#             ] + [subject_data.native_mask_path]\n",
    "#         tmp = nipype_mem.cache(spm.Normalize)(\n",
    "#             parameter_file=getattr(subject_data, \"parameter_file\"),\n",
    "#             apply_to_files=apply_to_files,\n",
    "#             write_bounding_box=[[-78, -112, -50], [78, 76, 85]],\n",
    "#             write_voxel_sizes=func_write_voxel_sizes,\n",
    "#             write_wrap=[0, 0, 0],\n",
    "#             write_interp=1,\n",
    "#             jobtype='write',\n",
    "#             ignore_exception=False,\n",
    "#             ).outputs.normalized_files\n",
    "\n",
    "#         subject_data.mask = hard_link(tmp[-1], subject_data.output_dir)\n",
    "#         subject_data.effects_maps = dict(zip(effects_maps.keys(), hard_link(\n",
    "#                     tmp[:-1], map_dirs[\"effects\"])))\n",
    "\n",
    "#         # warp anat image\n",
    "#         subject_data.anat = hard_link(nipype_mem.cache(spm.Normalize)(\n",
    "#                 parameter_file=getattr(subject_data, \"parameter_file\"),\n",
    "#                 apply_to_files=subject_data.anat,\n",
    "#                 write_bounding_box=[[-78, -112, -50], [78, 76, 85]],\n",
    "#                 write_voxel_sizes=anat_write_voxel_sizes,\n",
    "#                 write_wrap=[0, 0, 0],\n",
    "#                 write_interp=1,\n",
    "#                 jobtype='write',\n",
    "#                 ignore_exception=False,\n",
    "#                 ).outputs.normalized_files, subject_data.anat_output_dir)\n",
    "#     else:\n",
    "#         subject_data.mask = mask_path\n",
    "#         subject_data.effects_maps = effects_maps\n",
    "#         subject_data.z_maps = z_maps\n",
    "\n",
    "#     return subject_data\n",
    "\n",
    "\n",
    "\n",
    "# # fit GLM\n",
    "# def tortoise(*args):\n",
    "#     print(args)\n",
    "#     print(\n",
    "#         'Fitting a \"Fixed Effect\" GLM for merging LR and RL '\n",
    "#         'phase-encoding directions for subject %s ...' %\n",
    "#         subject_data.subject_id)\n",
    "#     fmri_glm = FMRILinearModel(subject_data.func,\n",
    "#                                [design_matrix.matrix\n",
    "#                                 for design_matrix in design_matrices],\n",
    "#                                mask='compute'\n",
    "#                                )\n",
    "#     fmri_glm.fit(do_scaling=True, model='ar1')\n",
    "#     print(\"... done.\\r\\n\")\n",
    "\n",
    "#     # save computed mask\n",
    "#     mask_path = os.path.join(subject_data.output_dir, \"mask.nii\")\n",
    "#     print(\"Saving mask image to %s ...\" % mask_path)\n",
    "#     nibabel.save(fmri_glm.mask, mask_path)\n",
    "#     print(\"... done.\\r\\n\")\n",
    "\n",
    "#     z_maps = {}\n",
    "#     effects_maps = {}\n",
    "#     map_dirs = {}\n",
    "#     try:\n",
    "#         for contrast_id, contrast_val in contrasts.items():\n",
    "#             print(\"\\tcontrast id: %s\" % contrast_id)\n",
    "#             z_map, eff_map = fmri_glm.contrast(\n",
    "#                 contrast_val,\n",
    "#                 con_id=contrast_id,\n",
    "#                 output_z=True,\n",
    "#                 output_effects=True\n",
    "#                 )\n",
    "\n",
    "#             # store stat maps to disk\n",
    "#             for map_type, out_map in zip(['z', 'effects'],\n",
    "#                                          [z_map, eff_map]):\n",
    "#                 map_dir = os.path.join(\n",
    "#                     subject_data.output_dir, '%s_maps' % map_type)\n",
    "#                 map_dirs[map_type] = map_dir\n",
    "#                 if not os.path.exists(map_dir):\n",
    "#                     os.makedirs(map_dir)\n",
    "#                 map_path = os.path.join(\n",
    "#                     map_dir, '%s_%s.nii' % (map_type, contrast_id))\n",
    "#                 print(\"\\t\\tWriting %s ...\" % map_path)\n",
    "\n",
    "#                 nibabel.save(out_map, map_path)\n",
    "\n",
    "#                 # collect zmaps for contrasts we're interested in\n",
    "#                 if map_type == 'z':\n",
    "#                     z_maps[contrast_id] = map_path\n",
    "\n",
    "#                 if map_type == 'effects':\n",
    "#                     effects_maps[contrast_id] = map_path\n",
    "\n",
    "#         return effects_maps, z_maps, mask_path, map_dirs\n",
    "#     except:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # for subject in subjects:\n",
    "# fwhm = preproc_params.get(\"fwhm\")\n",
    "# task_output_dir = os.path.join(os.path.dirname(subjects[0].output_dir))\n",
    "\n",
    "# kwargs = {\"regress_motion\": True,\n",
    "#           \"slicer\": slicer,\n",
    "#           \"threshold\": threshold,\n",
    "#           \"cluster_th\": cluster_th,\n",
    "#           \"protocol\": protocol,\n",
    "#           \"dc\": not preproc_params.get(\n",
    "#          \"disable_distortion_correction\", False),\n",
    "#           \"realign\": preproc_params[\"realign\"],\n",
    "#           \"coregister\": preproc_params[\"coregister\"],\n",
    "#           \"segment\": preproc_params[\"segment\"],\n",
    "#           \"normalize\": preproc_params[\"normalize\"],\n",
    "#           'func_write_voxel_sizes': preproc_params[\n",
    "#          'func_write_voxel_sizes'],\n",
    "#           'anat_write_voxel_sizes': preproc_params[\n",
    "#          'anat_write_voxel_sizes'],\n",
    "#           \"fwhm\": fwhm\n",
    "#           }\n",
    "\n",
    "# # n_jobs = int(os.environ.get('N_JOBS', 1))\n",
    "# # if n_jobs > 1:\n",
    "# #     subjects = Parallel(\n",
    "# #         n_jobs=n_jobs, verbose=100)(delayed(\n",
    "# #             run_suject_level1_glm)(\n",
    "# #                 subject_data,\n",
    "# #                 **kwargs) for subject_data in subjects)\n",
    "# # else:\n",
    "\n",
    "\n",
    "# # subjects = [run_suject_level1_glm(subject_data,\n",
    "# #                                       **kwargs)\n",
    "# #                 for subject_data in subjects]\n",
    "# # subjects = [subject for subject in subjects if subject]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   # # level 2\n",
    "    # stats_start_time = pretty_time()\n",
    "    # mask_images = [subject_data.mask for subject_data in subjects]\n",
    "    # group_mask = nibabel.Nifti1Image(\n",
    "    #     intersect_masks(mask_images).astype(np.int8),\n",
    "    #     nibabel.load(mask_images[0]).get_affine())\n",
    "    # nibabel.save(group_mask, os.path.join(\n",
    "    #         task_output_dir, \"mask.nii.gz\"))\n",
    "\n",
    "    # print(\"... done.\\r\\n\")\n",
    "    # print(\"Group GLM\")\n",
    "    # contrasts = subjects[0].contrasts\n",
    "    # subjects_effects_maps = [subject_data.effects_maps\n",
    "    #                          for subject_data in subjects]\n",
    "\n",
    "    # group_one_sample_t_test(\n",
    "    #     mask_images,\n",
    "    #     subjects_effects_maps,\n",
    "    #     contrasts,\n",
    "    #     task_output_dir,\n",
    "    #     threshold=threshold,\n",
    "    #     cluster_th=cluster_th,\n",
    "    #     start_time=stats_start_time,\n",
    "    #     subjects=[subject_data.subject_id for subject_data in subjects],\n",
    "    #     title='Group GLM for HCP fMRI %s protocol (%i subjects)' % (\n",
    "    #         protocol, len(subjects)),\n",
    "    #     slicer=slicer\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
